{
  "name": "Diversify Evaluation Metrics",
  "category": "mitigation-technique",
  "title": "diversify-evaluation-metrics",
  "id": 33,
  "caption": "Use additional evaluation metrics to determine whether model performance applies equally across all subgroups.",
  "description": "This technique expands evaluation beyond standard accuracy metrics to include fairness measures such as equalized odds, demographic parity, and calibration across subgroups. It involves conducting intersectional analysis of multiple demographic characteristics to uncover hidden biases. By measuring performance disparities across different populations, teams can identify and address systematic inequities in model behaviour.",
  "prompts": [
    "What fairness metrics are most relevant for your specific use case and potential harms?",
    "How will you balance competing metrics when trade-offs exist between different fairness criteria?",
    "What intersectional analysis will reveal biases not apparent from single demographic characteristics?"
  ],
  "example": "A job screening AI is evaluated not just on overall accuracy, but also on equal opportunity (true positive rates) and demographic parity across gender, ethnicity, and age groups. The analysis reveals the model recommends men 1.5 times more often than equally qualified women. Additional metrics like calibration show prediction confidence varies systematically by demographic group, leading to targeted model adjustments.",
  "icon": "diversify-evaluation-metrics-icon"
}
